# Intra-accounts disambiguation

## Table of Contents
- [Create virtual environment and install dependencies](#create-virtual-environment-and-install-dependencies)
- [Information on runs for dataset generation](#information-on-runs-for-dataset-generation)
  - [Kernel version](#kernel-version)
- [Step 1: Predict if a pair refers to the same entity](#step-1-predict-if-a-pair-refers-to-the-same-entity)
  - [Centralized runs](#centralized-runs)
    - [Character Bert](#character-bert)
    - [Spectrum Kernel](#spectrum-kernel)
  - [Federated learning with Fluke](#federated-learning-with-fluke)
    - [CharacterBert Full Model FL](#characterbert-full-model-fl)
    - [CharacterBert Partial FL 1](#characterbert-partial-fl-1)
    - [CharacterBert Partial FL 2](#characterbert-partial-fl-2)
    - [Spectrum Kernel with MLP](#spectrum-kernel-with-mlp)
    - [Spectrum Kernel with Logistic Regression](#spectrum-kernel-with-logistic-regression)
- [Step 2: Community detection](#step-2-community-detection)

## Create virtual environment and install dependencies
To create the virtual environment and install dependencies, you can use UV, a pyhton package manager. First, install UV:
```bash
curl -LsSf https://astral.sh/uv/install.sh | sudo sh
```
You can now execute the following commands:
```bash
uv venv
source .venv/bin/activate
uv sync
```

## Information on runs for dataset generation
To split the dataset into training and test sets, you can use the commands in the `create-dataset.zsh` file.  

Specifically, the split is performed by the `split_dataset.py` script, which allows reproducibility using a specific seed. The command used is:
```bash
uv run split_dataset.py --seed SEED
```
where `SEED` is the seed for reproducibility (default is 42). In our experiments, we used the following seeds: 23517, 47874, 9046, 67895, 81789.

The training set will be divided into four parts to create training sets for the four clients. After the split, each dataset created is preprocessed and saved in files named *__pp.csv_, using the `preprocessing.py` script. The command used is:
```bash
uv run preprocessing.py split-dataset SPLIT_DATASET_PATH
```
where `SPLIT_DATASET_PATH` is the path of the split dataset to be preprocessed.

All generated files will be saved in a folder named `./dataset/split_dataset_S$SEED/`, where `$SEED` indicates the seed used for splitting the dataset.

### Kernel version
In order to use Spectrum Kernel, you need to create datasets containing the similarities of the pairs. To do this, you can use the commands in the `create-sim-datasets-kernel.zsh` file.

In particular, the `kernel-classify.py` script with the `create-dataset` command creates the similarity datasets for the training set and the test set. Specifically, the command to use is as follows:
```bash
uv run kernel-classify.py create-dataset SEED [OPTIONS]
```
where:
- `SEED`: seed for reproducibility;
- `[OPTIONS]`:
    - `--n-features INT`: number of features to use (default: 7)
    - `--overwrite / --no-overwrite`: overwrites the existing similarity dataset (default: False)
    - `--use-bert / --no-use-bert`: uses an additional feature generated by CharacterBert (default: True)

To obtain them for each client's training sets, use the `create-clients-datasets` command. Specifically, the command to use is as follows:
```bash
uv run kernel-classify.py create-clients-datasets SEED NUM_CLIENTS [OPTIONS]
```
where:
- `SEED`: seed for reproducibility;
- `NUM_CLIENTS`: number of clients (in our experiments, the number of clients is always 4);
- `[OPTIONS]`:
    - `--n-features INT`: number of features to use (default: 7)
    - `--overwrite / --no-overwrite`: overwrites the existing similarity dataset (default: False)
    - `--use-bert / --no-use-bert`: uses an additional feature generated by CharacterBert (default: True)

The created files are saved in the `./dataset/` folder.

## Step 1: Predict if a pair refers to the same entity
We casted the problem of intra-accounts disambiguation to a binary classification task: instead of trying to learn how to group names together, we try to predict if a pair of names refers to the same entity (label 0) or not (label 1).

### Centralized runs

#### Character Bert
To run Character Bert model centrally, you can execute the following command:
```bash
uv run couple_prediction.py TRAIN_PATH TEST_PATH MODEL_NAME CONFIG_PATH NAME_LOG
```
where:
- `TRAIN_PATH`: path of the training set;
- `TEST_PATH`: path of the test set;
- `MODEL_NAME`: name of the model to use. It can be _"CBertClassif"_, _"CBertClassifFrz"_ or _"CBertClassifFrzSep"_;
- `CONFIG_PATH`: path of the configuration file. You can use the file `./config/parameters_FullCBert.json`;
- `NAME_LOG`: name for execution on Weights & Biases.


#### Spectrum Kernel
To run the Spectrum Kernel model centrally, you can execute the following command:
```bash
uv run kernel-classify.py <command> SEED [OPTIONS]
```
where:
- `<command>` is the name of the command to perform various functions. The command can be:
    - "`classify`": runs Spectrum Kernel with Logistic Regression;
    - "`nn-classify`": runs Spectrum Kernel with MLP;
- `SEED`: seed for reproducibility;
- `[OPTIONS]`: for the “`nn-classify`” command, you can specify:
    - `--bert` if you want to use the additional feature produced by CharacterBert, otherwise `--no-bert`. By default, it is not used.
    - `--client NUM_CLIENT` if you want to train only on the training set of a specific client. By default, it is None.

For both versions, you can also run the centralized version using Fluke with this command:
```bash
uv run fluke centralized EXP_FILE ALG_FILE
```
where:
- `EXP_FILE`: path to the experiment configuration file. If you want to run Spectrum Kernel with logistic regression, use `./config/exp_kernel_lr_centralized.yaml`, otherwise, if you want to run Spectrum Kernel with MLP, use `./config/exp_kernel_nn_centralized.yaml`.
- `ALG_FILE`: path to the algorithm configuration file. If you want to run Spectrum Kernel with logistic regression, use `./config/alg_kernel_lr.yaml`, otherwise, if you want to run Spectrum Kernel with MLP, use `./config/alg_kernel_nn.yaml`.

### Federated learning with Fluke

#### CharacterBert Full Model FL
In the Full Model FL, both CharacterBERT and the MLP classifier trained in a federated setting.

First, update the configuration file `./config/exp.yaml` with the seed used to split the dataset and change the execution log mode if necessary. Then, to perform federation via Fluke, use the following command:
```bash
uv run fluke federation ./config/exp.yaml ./config/alg.yaml
```

#### CharacterBert Partial FL 1
In the Partial FL 1, CharacterBERT kept frozen and only the classifier federated. 

First, update the configuration file `./config/exp_frozen.yaml` with the seed used to split the dataset and change the execution log mode if necessary. Then, to perform federation via Fluke, use the following command:
```bash
uv run fluke federation ./config/exp_frozen.yaml ./config/alg_frozen.yaml
```

#### CharacterBert Partial FL 2
In the partial FL 2, CharacterBERT locally fine-tuned while federating only the classifier.

First, update the configuration file `./config/exp_pretrained.yaml` with the seed used to split the dataset and change the execution log mode if necessary. Then, to perform federation via Fluke, use the following command:
```bash
uv run fluke federation ./config/exp_pretrained.yaml ./config/alg_pretrained.yaml
```

#### Spectrum Kernel with MLP
First, update the configuration file `./config/exp_kernel_nn.yaml`:
- replace the string “_SEED_” with the seed used to split the dataset;
- in the _data/dataset/_ section, set _bert_ to true if you want to use the embedding produced by CharacterBert as an additional feature, false otherwise;
- update the _logger_ section if necessary.

Then, to perform federation via Fluke, use the following command:
```bash
uv run fluke federation ./config/exp_kernel_nn.yaml ./config/alg_kernel_nn.yaml
```

#### Spectrum Kernel with Logistic Regression
First, update the configuration file `./config/exp_kernel_lr.yaml`:
- replace the string “_SEED_” with the seed used to split the dataset;
- in the _data/dataset/_ section, set _bert_ to true if you want to use the embedding produced by CharacterBert as an additional feature, false otherwise;
- update the _logger_ section if necessary.

Then, to perform federation via Fluke, use the following command:
```bash
uv run fluke federation ./config/exp_kernel_lr.yaml ./config/alg_kernel_lr.yaml
```


## Step 2: Community detection
After training, the resulting model can be used to generate pairwise predictions. For each account number, we then reconstruct the connected components graph to identify clusters of names and aliases corresponding to legal entities. Each unique name is represented as a separate node in the graph. Edges are established between pairs of names when the predicted label is 0, indicating that they refer to the same entity. This process results in a collection of graphs for each account number, with each graph representing a distinct cluster of aliases for a given entity. Additionally, within each cluster, we select a representative name by identifying the longest name among the nodes.

To do this, you must first set the configuration parameters in the `cluster_params.yaml` file. Finally, to run the script, use the following command:
```bash
uv run clustering.py <command> SEED WEIGHTS_PATH DATASET_PATH [OPTIONS]
```
where:
- `<command>` is the name of the command to perform various functions. The command can be:
    - "`cbert-accounts-disambiguation`": account disambiguation using CharacterBert model;
    - "`kernel-accounts-disambiguation`": account disambiguation using Spectrum Kernel model;
- `SEED`: seed for reproducibility;
- `WEIGHTS_PATH`: the path of weights;
- `DATASET_PATH`: the path of the dataset;
- `[OPTIONS]`:
    - `--name-wandb TEXT`: name for execution on Weights & Biases  

The output is saved in the folder specified in the configuration file. It will contain:
- `cluster.json`: a JSON file containing the following details for each IBAN:
    - _“IsShared”_: true label indicating whether the IBAN is shared or not;
    - “predicted_shared”: predicted label indicating whether the IBAN is shared or not;
    - “real_holders”: list containing the names of the true holders;
    - “holders”: list containing the clusters predicted from the connected components of the graph. It includes information on the name chosen to represent the cluster, the names of the entities included in the cluster, and the real name of the holder associated with the chosen name of the cluster.  
- `labeled_couple_dataset.csv`: a CSV file containing the following information for each pair generated from the original dataset: _"iban", "name1", "name2", "label", "IsShared", "predicted"_.
- `labeled_original_dataset.csv`: the original dataset labeled with predictions. It contains the following columns: _"index", "AccountNumber", "Name", "num occorrenze", "IsShared", "Holder", "cluster", "OldName", "IsShared_pred", "Predicted_Holder", "Representative_name"_.
- `log.txt`: files containing log information.
