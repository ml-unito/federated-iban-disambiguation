# Hyperparameters (HPs) of the algorithm
# Name of the algorithm: this must be the fully qualified name of the algorithm's class
# This means that you can define your own algorithm and use it here
# name: fluke.algorithms.fedavg.FedAVG
name: fluke.algorithms.fedavg.FedAVG
# Please refer to the algorithm's implementation to know which are its HPs
hyperparameters:
    # HPs of the clients
    client:
        # Batch size
        batch_size: 512
        # Number of local epochs
        epoch: 1
        # The loss function (loss name from torch.nn)
        loss: CrossEntropyLoss
        # HPs of the optimizer (the type of optimizer depends on the algorithm)
        optimizer:
            # if omitted, the default optimizer is SGD (optimizer name from torch.optim)
            name: AdamW
            lr: 0.01
            weight_decay: 0.0005
        # HPs of the scheduler (scheduler name from torch.optim.lr_scheduler)
        # this is optional
        scheduler:
            # if omitted, the default scheduler is StepLR
            name: StepLR
            gamma: 0.995
            step_size: 10
    # HPs of the server
    server:
        # Whether to weight the client's contribution
        # If not hyperparametera are needed set to {}
        weighted: true
    # The model (neural network) to federate.
    # This can be the name of a neural network included in `fluke.nets` or
    # the fully qualified name of an user defined class
    # model: lib.CBertClassif.CBertClassif
    model: lib.CBertClassifFrz.CBertClassifFrz